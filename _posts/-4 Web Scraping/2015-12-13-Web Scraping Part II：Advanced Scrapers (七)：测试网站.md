---
layout: post
title: Web Scraping Part II：Advanced Scrapers (七)：测试网站
categories: [Web Scraping]
tags: [Web Scraping, BeaufitulSoup]
number: [-4.1.13]
fullview: false
shortinfo: 本文是基于Ryan Mitchell的《Web Scraping With Pyhton》书本的第二部分Advanced Scraper的第1篇笔记，。

---
目录
{:.article_content_title}


* TOC
{:toc}

---
{:.hr-short-left}


在《Web Scraping With Python》第一部分，Basic Srapers，我们覆盖了Web Scraping的基础部分，即如何获取数据，解析数据和存储数据。之所以说它是基础，是因为获取的数据都是整理好存储在既定格式(如html，xml，json，doc，txt，pdf，csv等)里的；并且我们没有涉及到反爬虫程序(antiscraping measures)，JavaScript，登录表格，流数据等话题。第二部分，Advanced Scraper，我们就来关注这些Advanced的话题。

首先，本文作为第二部分Advanced Scraper的第1篇笔记，我们来了解下如何从将**原始数据**清理，规范化以成为我们需要的数据，即**数据清理**。

## 1 总结数据 ##


## 2 Markov Models ##


## 3 Natural Language Toolkit ##

### 3.1 安装和设置 ###

### 3.2 NTLTK概率分析 ###

### 3.3 NLTK词典学分析 ###

## 4 其他资源 ##

## 5 总结 ##

[Cleaning in code]({{ site.baseurl}}/web%20scraping/2015/12/07/Web-Scraping-Part-II-Advanced-Scrapers-(一)-数据清理.html#cleaining-in-code)

{: .img_middle_mid}
![web scraping](/assets/images/posts/2015-12-07/Data Cleaning Summary.png)

{% highlight python linenos %}

{% endhighlight %}

## 6 参考资料 ##

- [《BeautifulSoup Documentation》](https://www.crummy.com/software/BeautifulSoup/bs4/doc/);
- [《Python 3 Documentation》](https://docs.python.org/3/);
- [《OpenRefine》](http://openrefine.org/);



